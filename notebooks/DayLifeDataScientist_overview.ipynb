{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day in the Life of a Data Scientist\n",
    "### J. Andrew Howe, PhD\n",
    "\n",
    "- <a href=#bapara>BAPARA</a>\n",
    "- <a href=#acquire>**Acquire**</a>\n",
    "- <a href=#prepare-explore>**Prepare**-Explore</a>\n",
    "- <a href=#prepare-preprocess>**Prepare**-Preprocess</a>\n",
    "- <a href=#analyze-hypothesize>**Analyze**-Hypothesize</a>\n",
    "- <a href=#analyze-select>**Analyze**-Select</a>\n",
    "- <a href=#analyze-build>**Analyze**-Build</a>\n",
    "    - <a href=#linreg>Multiple Linear Regression</a>\n",
    "    - <a href=#dectree>Decision Tree Regression</a>\n",
    "    - <a href=#nn>Simlple Neural Network</a>\n",
    "- <a href=#analyze-assess>**Analyze**-Assess</a>\n",
    "- <a href=#report-act>**Report**</a>\n",
    "- <a href=#report-act>**Act**</a>\n",
    "- <a href=#end>The End</a>\n",
    "\n",
    "<a id=top></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will summarize the basic process and several of the stages of the data science process, essentially capturing a day in the life of a data scientist or statistical modeler.\n",
    "\n",
    "We use a simple \"toy\" dataset for demonstrative purposes, without loss of much generality.\n",
    "\n",
    "[This notebook](https://github.com/ahowe42/MachineLearningKernelsDemo) Goes into much more detail on several machine learning concepts:\n",
    "- Exploratory Data Analysis & Visualization\n",
    "- Feature Engineering\n",
    "- Supervised Classification Modeling\n",
    "- Logistic Regression\n",
    "- Numeric Optimization\n",
    "- Support Vector Machines\n",
    "- Reproducing Kernel Hilbert Spaces\n",
    "- Cross-Validation\n",
    "- Hyperparameter Tuning\n",
    "- Feature Selection\n",
    "- Feature Hierarchy\n",
    "- Genetic / Evolutionary Algorithms\n",
    "- Information Theoretic Criteria\n",
    "- Ensemble Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Definitions\n",
    "<a id=impdef></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ipdb\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from captum.attr import DeepLift, IntegratedGradients, NoiseTunnel\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit, ShuffleSplit\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import chart_studio.plotly as ply\n",
    "import plotly.offline as plyoff\n",
    "import plotly.graph_objects as go\n",
    "import plotly.subplots as plysub\n",
    "import plotly.express as px\n",
    "import plotly.tools as plytool\n",
    "\n",
    "# to use plotly offline, need to initialize with a plot\n",
    "plyoff.init_notebook_mode(connected=True)\n",
    "init = go.Figure(data=[go.Scatter({'x':[1, 2], 'y':[42, 42], 'mode':'markers'})],\n",
    "                 layout=go.Layout(width=42, height=42, title='Init', xaxis={'title':'x'}))\n",
    "plyoff.iplot(init)\n",
    "\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_colwidth = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define my own RMSE metric\n",
    "def RMSE(y_true, y_pred):\n",
    "    return math.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plotting function for correlations\n",
    "def correlationsPlot(rhoYourBoat, plotTitl='Feature Correlations Plot', trcLims=(0.0, 1.0), tweaks=(20, None, None, 1.1)):\n",
    "    '''\n",
    "    This creates and returns a bubble plot visualization of a correlation matrix. The\n",
    "    sizes of the bubbles are proportional to the absolute magnitude of the correlations.\n",
    "    Positive correlations are only plotted in the upper triangle, with colors ranging\n",
    "    from green (0) to red (+1). Negative correlations are plotted in the lower triangle,\n",
    "    with colors ranging from green (0) to blue (-1). Perfect correlations are indicated\n",
    "    with black bubbles.\n",
    "    :param rhoYourBoat: dataframe of correlation matrix, probably created with pd.DataFrame.corr()\n",
    "    :param plotTitl: optional (default='Feature Correlations Plot') plot title\n",
    "    :param trcLims: (default=(0.0,1.0)) = ordered tuple of \"buckets\" in which to place absolute correlations for\n",
    "        plotting traces; must include at least 0 and 1\n",
    "    :param tweaks (default=(20,None,None,1.1)): tuple of position & size tweaking values for plotly; maximum size of\n",
    "        bubbles, plot width, plot height, y position of legend\n",
    "    :return fig: plotly correlation plot figure\n",
    "    '''\n",
    "\n",
    "    # set the granulatrity of the colors\n",
    "    n = 101  # must be odd so in the middle at correlation = 0 is just green\n",
    "\n",
    "    # number features\n",
    "    p = len(rhoYourBoat.columns)\n",
    "    ps = list(range(p))\n",
    "\n",
    "    # positive correltions are red>green\n",
    "    scl = np.linspace(1.0, 0.0, n)\n",
    "    redsP = np.round(255 * scl)\n",
    "    grnsP = 255 - redsP\n",
    "    blusP = [0.0] * n\n",
    "\n",
    "    # negative correlations are blue>green\n",
    "    scl = scl[:-1]\n",
    "    blusN = np.round(255 * scl)\n",
    "    grnsN = 255 - blusN\n",
    "    redsN = [0.0] * n\n",
    "\n",
    "    # adding 2 more to make the endpoints for perfect correlations\n",
    "    scl = np.linspace(-1.0, 1.0, 2 * n - 1 + 2)\n",
    "\n",
    "    # make the colormap - perfectly uncorrelated and perfectly correlated are black\n",
    "    rgb = ['rgb(0,0,0)']\n",
    "    rgb.extend(['rgb(%d,%d,%d)' % (r, g, b) for r, g, b in\n",
    "                zip(np.r_[redsN, redsP[::-1]], np.r_[grnsN, grnsP[::-1]], np.r_[blusN, blusP[::-1]])])\n",
    "    rgb.append('rgb(0,0,0)')\n",
    "\n",
    "    # now map correlations to colors - unhappy that I have to do this double loop :-(\n",
    "    vals = rhoYourBoat.values\n",
    "    cols = np.zeros(shape=vals.shape, dtype=object)\n",
    "    for i in ps:\n",
    "        for j in ps:\n",
    "            v = vals[i, j]\n",
    "            mni = np.argmin(np.abs(v - scl))\n",
    "            mnv = scl[mni]\n",
    "            cols[i, j] = rgb[mni]\n",
    "            # print('%0.5f,%d,%0.5f,%s'%(v,mni,mnv,cols[i,j]))\n",
    "\n",
    "    # filter data so the upper triangle is (+) correlations and lower triangle is (-) correlations\n",
    "    y = np.tile(ps, (p, 1))\n",
    "    x = y.T\n",
    "    x = x.flatten()\n",
    "    y = y.flatten()\n",
    "    vals = vals.flatten()\n",
    "    cols = cols.flatten()\n",
    "    keepind = ((y > x) & (vals > 0)) | ((x > y) & (vals < 0))\n",
    "    x = x[keepind]\n",
    "    y = y[keepind]\n",
    "    vals = vals[keepind]\n",
    "    cols = cols[keepind]\n",
    "    absVals = np.abs(vals)\n",
    "\n",
    "    # set a minimum bubble size\n",
    "    minBub = 0.09 * tweaks[0]\n",
    "\n",
    "    # put together the figure - make multiple traces\n",
    "    trc = [go.Scatter(\n",
    "        {'x': ps, 'y': ps, 'mode': 'lines', 'line': {'color': 'black'}, 'showlegend': False, 'hoverinfo': 'skip'})]\n",
    "    for i, t in enumerate(trcLims):\n",
    "        # build the index for the traces\n",
    "        if i == 0:\n",
    "            continue\n",
    "        elif i == 1:\n",
    "            indx = (absVals <= t) & (absVals >= trcLims[0])\n",
    "            trcName = '$\\\\vert\\\\rho\\\\vert\\\\in[%0.2f,%0.2f]$' % (trcLims[0], t)\n",
    "        else:\n",
    "            indx = (absVals <= t) & (absVals > trcLims[i - 1])\n",
    "            trcName = '$\\\\vert\\\\rho\\\\vert\\\\in(%0.2f,%0.2f]$' % (trcLims[i - 1], t)\n",
    "        # create & add the trace\n",
    "        trc.append(go.Scatter({'x': x[indx], 'y': y[indx], 'mode': 'markers', 'text': ['%0.4f' % v for v in vals[indx]],\n",
    "                               'name': trcName, 'hoverinfo': 'x+y+text',\n",
    "                               'marker': {'color': cols[indx], 'line': {'color': cols[indx]},\n",
    "                                          'size': np.maximum(tweaks[0] * absVals[indx], minBub)}}))\n",
    "\n",
    "    # finalize the layout\n",
    "    lout = go.Layout({'title': plotTitl, 'legend': {'orientation': 'h', 'x': 0, 'y': tweaks[-1]},\n",
    "                      'xaxis': {'ticklen': 1, 'tickvals': ps, 'ticktext': rhoYourBoat.columns.values,\n",
    "                                'mirror': True, 'showgrid': False, 'range': [-1, p], 'linecolor': 'black',\n",
    "                                'linewidth': 0.5, 'zeroline': False, 'tickangle': 90},\n",
    "                      'yaxis': {'ticklen': 1, 'tickvals': ps, 'ticktext': rhoYourBoat.index.values,\n",
    "                                'mirror': True, 'showgrid': False, 'range': [-1, p], 'linecolor': 'black',\n",
    "                                'linewidth': 0.5, 'zeroline': False}})\n",
    "    if tweaks[1] is not None:\n",
    "        lout['width'] = tweaks[1]\n",
    "    if tweaks[2] is not None:\n",
    "        lout['height'] = tweaks[2]\n",
    "\n",
    "    return go.Figure(data=trc, layout=lout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plotting function for common regression diagnostic plots\n",
    "def ResultsPlots(data, sequenceCol, responseCol, predCol, resdCol, colorCol, overall_title, plot_colors=('red',)*4):\n",
    "    '''\n",
    "    This creates and returns a quad-plot of results from a model. The four plots are: a) prediction vs response,\n",
    "    b) histogram of residuals, c) residuals by sequence, d) residuals by response. They are arranged as\n",
    "    [[a,b],[c,d]].\n",
    "    :param data: dataframe holding the sequence, response, prediction, and residual columns\n",
    "    :param sequenceCol: column in the dataframe holding the time or sequence counter; if None, a counter will be used\n",
    "    :param responseCol: column in the dataframe holding the response variable\n",
    "    :param predCol: column in the dataframe holding the model predictions\n",
    "    :param resdCol: column in the dataframe holding the model residuals\n",
    "    :param colorCol: optional column in the dataframe holding the color for each observation for the plots\n",
    "    :param overall_title: title to go on top of the quad plot\n",
    "    :param plot_colors (default=(red, red, red, red)): optional tuple of colors for each plot; only used if colorCol\n",
    "    \tnot passed\n",
    "    :return fig: plotly plot figure\n",
    "    '''\n",
    "    \n",
    "    # copy the input dataframe\n",
    "    data = data.copy(deep=True)\n",
    "    \n",
    "    if colorCol is None:\n",
    "    \tcolorCol = 'NOCOLOR'\n",
    "    \n",
    "    # setup the subplot\n",
    "    figRes = plysub.make_subplots(rows=2, cols=2, subplot_titles=['Predictions vs Responses', 'Residuals Distribution','Residuals by Sequence', 'Residuals vs Responses'])\n",
    "    \n",
    "    # for the actual vs preds plot, build the fit line\n",
    "    actual = data[responseCol].values.reshape(-1,1)\n",
    "    lindat = np.linspace(actual.min(), actual.max(), 10).reshape(-1, 1)\n",
    "    fitlin = LinearRegression(n_jobs=-1)\n",
    "    fitlin.fit(X=actual, y=data[predCol])\n",
    "    actpred = fitlin.predict(X=lindat)\n",
    "    r2 = fitlin.score(X=actual, y=data[predCol])\n",
    "    # create the trace and annotation\n",
    "    r2trc = go.Scatter(x=lindat.squeeze(), y=actpred, mode='lines', name='fit', line={'color':'black','width':1}, showlegend=False)\n",
    "    r2ann = dict(x=lindat[5][0], y=actpred[5], xref='x1', yref='y1', text='$\\\\rho=%0.3f$'%(r2), showarrow=False, bgcolor='#ffffff')\n",
    "    \n",
    "    # actuals vs resids plot\n",
    "    if colorCol == 'NOCOLOR':\n",
    "        data[colorCol] = plot_colors[0]\n",
    "    figRes.add_trace(go.Scatter(x=data[responseCol], y=data[predCol], mode='markers', marker={'color':data[colorCol]}, showlegend=False), 1,1)\n",
    "    figRes.add_trace(r2trc, 1, 1)\n",
    "    figRes['layout']['xaxis1'].update(title=responseCol)\n",
    "    figRes['layout']['yaxis1'].update(title=predCol)\n",
    "    \n",
    "    # residuals histogram\n",
    "    if colorCol == 'NOCOLOR':\n",
    "        data[colorCol] = plot_colors[1]\n",
    "    figRes.add_trace(go.Histogram(x=data[resdCol], histnorm='', marker={'color':data[colorCol]}, showlegend=False), 1,2)\n",
    "    figRes['layout']['xaxis2'].update(title=resdCol)\n",
    "    figRes['layout']['yaxis2'].update(title='count')\n",
    "    \n",
    "    # get the time variable\n",
    "    if sequenceCol is None:\n",
    "        seq = list(range(len(data)))\n",
    "        seqNam = 'sequence'\n",
    "    else:\n",
    "        seq = data[sequenceCol]\n",
    "        seqNam = sequenceCol\n",
    "    \n",
    "    # residuals by time plot\n",
    "    if colorCol == 'NOCOLOR':\n",
    "        data[colorCol] = plot_colors[2]\n",
    "    figRes.add_trace(go.Scatter(x=seq, y=data[resdCol], mode='markers', marker={'color':data[colorCol]}, showlegend=False), 2, 1)\n",
    "    figRes['layout']['xaxis3'].update(title=seqNam)\n",
    "    figRes['layout']['yaxis3'].update(title=resdCol)\n",
    "    \n",
    "    # residuals by response plot\n",
    "    if colorCol == 'NOCOLOR':\n",
    "        data[colorCol] = plot_colors[3]\n",
    "    figRes.add_trace(go.Scatter(x=data[responseCol], y=data[resdCol], mode='markers', marker={'color':data[colorCol]}, showlegend=False), 2,2)\n",
    "    figRes['layout']['xaxis4'].update(title=responseCol)\n",
    "    figRes['layout']['yaxis4'].update(title=resdCol)\n",
    "    \n",
    "    # update layout\n",
    "    figRes['layout'].update(title=overall_title, height=1000)\n",
    "    anns = list(figRes['layout']['annotations'])\n",
    "    anns.append(r2ann)\n",
    "    figRes['layout']['annotations'] = anns\n",
    "    \n",
    "    return figRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the model class for a neural net with variable hidden layers & dropout\n",
    "class myNN(nn.Module):\n",
    "    def __init__(self, layerNodes, wInit, pDropout, activations):\n",
    "        super(myNN, self).__init__()\n",
    "        self.activations = activations[1:]\n",
    "        self.len = len(layerNodes)-1\n",
    "        self.drop = nn.Dropout(pDropout)\n",
    "        self.linears = nn.ModuleList()\n",
    "        for I, O in zip(layerNodes, layerNodes[1:]):\n",
    "            # create the layer\n",
    "            lin = nn.Linear(I, O)\n",
    "            # initialize it\n",
    "            nn.init.zeros_(lin.bias)\n",
    "            if wInit == 'uni':\n",
    "                nn.init.uniform_(lin.weight)\n",
    "            elif wInit == 'xav':\n",
    "                nn.init.xavier_uniform_(lin.weight)\n",
    "            elif wInit == 'kai':\n",
    "                nn.init.kaiming_uniform_(lin.weight, nonlinearity='relu')\n",
    "            # and now add it\n",
    "            self.linears.append(lin)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, (L, A) in enumerate(zip(self.linears, self.activations)):\n",
    "            # dropout if not the output layer\n",
    "            if i <= self.len - 1:\n",
    "                x = self.drop(L(x))\n",
    "            else:\n",
    "                x = L(x)\n",
    "            # compute the activation\n",
    "            if A == 'relu':\n",
    "                x = nn.functional.relu(x)\n",
    "            elif A == 'sigmoid':\n",
    "                x = torch.sigmoid(x)\n",
    "            elif A == 'leakyrelu':\n",
    "                x = nn.functional.leaky_relu(x)\n",
    "            elif A == 'tanh':\n",
    "                x = torch.tanh(x)\n",
    "        return x\n",
    "    \n",
    "    def __str__(self):\n",
    "        mn = super(myNN, self).__str__()\n",
    "        return '%s\\nActivations: %s'%(mn, self.activations)\n",
    "    \n",
    "    def show(self):\n",
    "        print('Weights')\n",
    "        for lin in self.linears:\n",
    "            print(lin.weight.detach().numpy())\n",
    "        print('Biases')\n",
    "        for lin in self.linears:\n",
    "            print(lin.bias.detach().numpy())        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to train an nn, based on a loss and optimizer\n",
    "def trainNN(data, network, loss, optimizer, scheduler, epochs, writer, randState=None, talkFreq=0.2):\n",
    "    '''\n",
    "    Train a neural network specified by a network, optimizer, and loss, fitting to data from\n",
    "    a training dataloader, and evaluating on a testing dataloader. NB This procedure updates\n",
    "    the input network *in place*.\n",
    "    :param data: list holding the training set data loader and testing set dataloader. The first\n",
    "        may load in batches, while the second may not\n",
    "    :param network: PyTorch NN architecture as defined to extend the nn.Module class, or using\n",
    "        the Sequential constructor\n",
    "    :param loss: PyTorch loss function\n",
    "    :param optimizer: PyTorch optimizer\n",
    "    :param scheduler: PyTorch learning rate decay scheduler\n",
    "    :param epochs: integer number of epochs\n",
    "    :param writer: PyTorch TensorBoard SummaryWriter object\n",
    "    :param randState: optional seed for PyTorch psuedo random number generator\n",
    "    :param talkFreq: optional (default=0.2) frequency with which progress should be printed\n",
    "    :return trnLoss: loss on training set per epoch\n",
    "    :return tstLoss: loss on testing set per epoch\n",
    "    '''\n",
    "    \n",
    "    # set the prng seed, maybe\n",
    "    if randState:\n",
    "        torch.manual_seed(randState)\n",
    "        writer.add_scalar('Params/random_state', randState, 0)\n",
    "        \n",
    "    # save the learning rate\n",
    "    writer.add_scalar('Params/initial_learning_rate', optimizer.param_groups[0]['initial_lr'], 0)\n",
    "    \n",
    "    # add the model graph\n",
    "    try:\n",
    "        writer.add_graph(network, data[0].dataset.x)\n",
    "    except TypeError as err:\n",
    "        print(\"Network may be from nn.Sequential, so can't be added to TensorBoard!\")\n",
    "        \n",
    "    # get the data loaders\n",
    "    trn, tst = data\n",
    "    \n",
    "    # containers for training / testing loss\n",
    "    trnLoss = [np.inf]*epochs\n",
    "    tstLoss = [np.inf]*epochs\n",
    "\n",
    "    # iterate over epochs\n",
    "    for epoch in range(epochs):\n",
    "        # train with minibatch gradient descent\n",
    "        network.train(True) # setting train to True tells pytorch that ops like dropout / batch normalization to occur, which wouldn't occur during evaluation\n",
    "        # iterate over batches\n",
    "        for indx, (x, y) in enumerate(trn):\n",
    "            # forward step\n",
    "            yhat = network(x) # implements forward propagation as implemented by the forward() method\n",
    "            # compute loss (not storing for now, will do after minibatching)\n",
    "            l = loss(yhat, y)\n",
    "            # backward step\n",
    "            optimizer.zero_grad() # set gradients to zero before backprop, so there's no accumulation among batches\n",
    "            l.backward() # backward propagation computes the gradients of the parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "        # update the learning rate\n",
    "        scheduler.step()\n",
    "        writer.add_scalar('Params/learning_rate', scheduler.get_last_lr()[0], epoch)\n",
    "        \n",
    "        # evaluate performance\n",
    "        with torch.no_grad():\n",
    "            network.train(False) # using no_grad() and train() false turns off any gradient updating or special functionality\n",
    "            # evaluate loss on training set\n",
    "            yhat = network(trn.dataset.x)\n",
    "            trnLoss[epoch] = loss(yhat, trn.dataset.y)\n",
    "            # evaluate loss on testing set\n",
    "            yhat = network(tst.dataset.x)\n",
    "            tstLoss[epoch] = loss(yhat, tst.dataset.y)\n",
    "            # tensorboard\n",
    "            writer.add_scalar('Train/Loss', trnLoss[epoch], epoch)\n",
    "            writer.add_scalar('Test/Loss', tstLoss[epoch], epoch)\n",
    "            # maybe talk\n",
    "            if epoch % (epochs*talkFreq) == 0:\n",
    "                print('Epoch %d Training (Testing) Loss = %0.2f (%0.2f)'%(epoch, trnLoss[epoch], tstLoss[epoch]))\n",
    "\n",
    "    print('==========\\nTraining Initial Loss = %0.2f, Final Loss = %0.2f'%(trnLoss[0], trnLoss[-1]))\n",
    "    print('Testing Initial Loss = %0.2f, Final Loss = %0.2f'%(tstLoss[0], tstLoss[-1]))\n",
    "    \n",
    "    # return results\n",
    "    return trnLoss, tstLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the data class\n",
    "class Data(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.FloatTensor(x)\n",
    "        self.y = torch.Tensor(y.astype(int))\n",
    "        self.len, self.p = self.x.shape\n",
    "    def __getitem__(self, index):      \n",
    "        return self.x[index], self.y[index]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BAPARA\n",
    "For analytics projects, we follow a modification of the most widely-used process model: [***CRISP-DM***](https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining). CRISP-DM is a high-level process model that iterates over 6 phases, shown here:<br><img src=\"./CRISP-DM_Process_Diagram.png\" alt=\"CRISP-DM\" width=\"300\" height=\"300\"><br>A fundamental component that is often overlooked while performing analytics, or at least only included implicitly, is business understanding. The extent to which any analytics project is impactful (*\"moves the needle\"*, according to our founder Preston Cody) is highly dependent on accurate understanding of the business environment and need. Digging deeper into this high-level process model - which also specifies lower-level stages - the Data Understanding through Evaluation phases often iterate and loop back more than is shown by the CRISP-DM process diagram.\n",
    "\n",
    "Our modification of CRISP-DM is called ***BAPARA***; the process is composed of 13 segments of 6 phases, defined below, which can loop back at multiple points in the process, as shown here:<br><img src=\"BAPARA_Process_DiagramW.png\" alt=\"BAPARA\" height=\"200\">\n",
    "\n",
    "In the remainder of this discussion, we will detail and demonstrate most of these phases.\n",
    "\n",
    "<a href=#top>Go to top</a>\n",
    "<a id=bapara></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "Accurate and complete business and domain understanding is necessary for almost all segments of the BAPARA process.\n",
    "- Embed an SME: in addition to technical and statistical expertise, project success relies on embedding a true \"user\" who is intimately involved with the impacted business workflow\n",
    "- Explore Problem Space: business / workflow context of the problem to be solved and desired outcomes of the project must be defined; initial list of potential hypotheses to test can be determined; project success should be defined as quantitatively [***SMART***](https://en.wikipedia.org/wiki/SMART_criteria) as possible\n",
    "- *Success Looks Like: The analytics team is staffed with a user, can articulate the desired outcomes, and has an initial list of hypotheses to consider. Can state a summary value proposition in the form of \"For X, who want to Y, so they can Z\".*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire - Identify, collect, and integrate relevant data\n",
    "- Identify: consult with SMEs, seek out what is actually available, subject to constraints\n",
    "- Collect: obtain appropriate access credentials, query structured / unstructured databases, access through business data warehouses, connect to web / network APIs, store raw accessed data - perhaps in a data lake or simple flat files\n",
    "- Integrate: identify uniqueness and matching keys (primary & foreign keys in RDBMS parlance), join data sources appropriately (inner join, outer join, one-to-one, one-to-many, many-to-many)\n",
    "\n",
    "<a href=#top>Go to top</a>\n",
    "<a id=acquire></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Many real-world data science problems require multiple datasets which must be joined, standardized, and jointly validated.\n",
    "- Data can come from many sources: standalone files, automated feeds, databases, etc.\n",
    "- Data can come in multiple formats as well: structured, geospatial, text, etc.\n",
    "- Data acquisition & preparation generally taken to be about **80%** of a data science project.\n",
    "\n",
    "For demonstrative purposes, we're just using the well-known Boston Housing dataset; this dataset is often used to test and benchmark performance on machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load and view the data\n",
    "res = load_boston()\n",
    "data = pd.DataFrame(data=np.c_[res['data'], res['target']], columns=res['feature_names'].tolist()+res.get('target_names', ['MedianHousePrice']))\n",
    "(n, p) = data.shape\n",
    "print(res['DESCR'])\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly permute observations, in case there is some order in the data\n",
    "'''prng = 42\n",
    "data = data.sample(frac=1.0, random_state=prng)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set data types - feature names are numpy.str instead of string, for some reason\n",
    "cateFeatures = ['CHAS']\n",
    "contFeatures = [str(feat) for feat in res['feature_names'] if feat not in cateFeatures]\n",
    "response = 'MedianHousePrice'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare - Explore, clean, and pre-process acquired data\n",
    "### Explore: descriptive statistics (probability distribution, centrality, dispersion, general morphology), visualizations, validation wrt domain expertise, evaluation of relationships between features (pairwise linear correlation, multiple regression), grouping structures, category frequencies & degree of imbalance; preparation of publication-ready statistical tables and data visualizations\n",
    "\n",
    "<a href=#top>Go to top</a>\n",
    "<a id=prepare-explore></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check descriptive statistics\n",
    "print('Descriptive Statistics')\n",
    "display(data[contFeatures].describe(percentiles=[0.05, 0.25, 0.5, 0.75, 0.95]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- scales: there is a wide range of scales between features; this should be handled by standardizing or normalizing features\n",
    "- **CRIM**: the mean is much higher than the median, indicating positive skew or possible outliers; this is also seen in the difference between the 95th percentile and the max\n",
    "- **ZN**: there is also a suggestion of skew or possible outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check correlations\n",
    "print('Correlations')\n",
    "display(data[['MedianHousePrice']+contFeatures].corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- MedianHousePrice is only moderately *linearly* correlated with **RM** (positive) and **LSTAT** (negative)\n",
    "- MedianHousePrice is only weakly correlated (linearly) with **AGE**, which is surprising\n",
    "- It's not really easy to find high correlations between features in this table\n",
    "- Also note that this only identifies *linear* correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# categorical value counts\n",
    "for feat in cateFeatures:\n",
    "    print('%s Value Counts'%feat)\n",
    "    featn = len(data[feat].values)\n",
    "    vc = pd.DataFrame(data[feat].value_counts()).rename(columns={feat:'Frequency'})\n",
    "    vc['Relative Frequency'] = vc['Frequency']/featn\n",
    "    display(vc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- There is a high amount of imbalance in the Charles River flag feature.\n",
    "- If **CHAS** is ultimately used in modeling, this imbalance will need to be taken into consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' 5 x 3 subplots of histograms '''\n",
    "# setup\n",
    "fig = plysub.make_subplots(rows=5, cols=3, print_grid=False, subplot_titles=data.columns)\n",
    "rowcols = [(math.floor(i/3)+1, i%3+1) for i in range(len(data.columns))]\n",
    "\n",
    "# add traces\n",
    "for (ind, feat) in enumerate(data.columns):\n",
    "    r, c = rowcols[ind]\n",
    "    fig.add_traces(go.Histogram(x=data[feat], histnorm='probability density'), r, c)\n",
    "\n",
    "# update layout\n",
    "fig['layout'].update({'title':'Feature Distributions', 'showlegend':False, 'height':1200})\n",
    "\n",
    "# plot\n",
    "plyoff.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "- **CRIM**: highly skewed positively, as suggested by descriptive statistics; most places have very low CRIM rates\n",
    "- **ZN**: similarly skewed, with most areas having very low proportion of residences\n",
    "- **RAD**: small percent of very high values; possible outliers to investigate\n",
    "- **TAX**: tax rates show possible bimodal behavior; could perhaps be related to **CHAS**; need to investigate further\n",
    "- **MedianHousePrice**: small group of very high-price regions; need to investigate if these are related to a feature; geospatial location could probably help predict this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' scatter plot matrix '''\n",
    "# setup traces\n",
    "trcs = [go.Splom(dimensions=[{'label':col, 'values':data.loc[data['CHAS']==0, col].values} for col in data.columns],\n",
    "                showupperhalf=False, diagonal_visible=False, marker={'color':'red', 'opacity':0.4}, name='CHAS==0'),\n",
    "        go.Splom(dimensions=[{'label':col, 'values':data.loc[data['CHAS']==1, col].values} for col in data.columns],\n",
    "                showupperhalf=False, diagonal_visible=False, marker={'color':'green', 'opacity':0.4}, name='CHAS==1')]\n",
    "fig = go.Figure(data=trcs, layout=go.Layout(title='Scatterplot Matrix', showlegend=True, height=1400))\n",
    "\n",
    "# plot\n",
    "plyoff.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "- a few curvilinear relationships between features visible: **MedianHousePrice** vs **LSTAT** and **DIS** vs **NOX** (possibly **INDUS**); these quadratic relationships can't be picked up by a linear model, without preprocessing of data\n",
    "- only a few features seems to have linear (or nearly so) relationships with response - and these tend to have a lot of noise: **RM** and **NOX**\n",
    "- some features also seem to have a relationship with the Charles River flag feature (see coloring): **DIS**, **B**, **CRIM**\n",
    "- **MedianHousePrice**: would have expected grouping of prices by **CHAS**, but range of prices for regions not bound by the river entirely covers the range of prices for regions on the Charles river"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' correlations plot '''\n",
    "fig = correlationsPlot(data[['MedianHousePrice']+contFeatures].corr(), plotTitl='Feature Correlations Plot', trcLims=(0.0, 0.5, 0.75, 0.9, 0.95, 1.0), tweaks=(20, None, 1200, 1.1))\n",
    "plyoff.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- Most features are only slightly linearly correlated.\n",
    "- **DIS** and **NOX** have a moderately negative correlation.\n",
    "- **INDUS** and **NOX** have a moderately positive correlation.\n",
    "- **RAD** and **TAX** have a strong positive correlation - this might be strong enough for some people to drop one; however, reviewing the scatter plot matrix, this is likely due to the correlation fitting a line up to the single point that is an outlier for both features, so this is spurious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean: anomaly / outlier detection & handling, management of missing data (drop feature, drop rows, impute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*skipping this segment*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process - feature selection, feature extraction, feature engineering, normalization, encoding, discretization, interactions, other transformations; feature engineering should make use of domain knowledge to generate new features based on physical phenomena and relationships between features\n",
    "\n",
    "<a href=#top>Go to top</a>\n",
    "<a id=prepare-preprocess></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "- Lacking any domain knowledge (other than *location, location, location*), we can't perform any \"smart\" features engineering.\n",
    "- Given the characteristics observed in the plots, we see a reason to engineer 2 kinds of features:\n",
    "    - interaction features\n",
    "    - squared features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' engineer higher-order features '''\n",
    "# add squares and interactions for continuous features\n",
    "highords = []\n",
    "for col in contFeatures:\n",
    "    # squares\n",
    "    highords.append(col+'_sq')\n",
    "    data[highords[-1]] = data[col]**2\n",
    "    # interactions\n",
    "    for col2 in res['feature_names']:\n",
    "        # > test to ensure we don't get both A_B and B_A\n",
    "        if col2 > col:\n",
    "            highords.append(col+'_'+col2)\n",
    "            data[highords[-1]] = data[col]*data[col2]\n",
    "    \n",
    "contFeatures = contFeatures + highords\n",
    "# reorder columns\n",
    "data = data[contFeatures+cateFeatures+[response]]\n",
    "# talk\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- We created all interactions and all squared features, but all are probably not necessary; likely necessary higher-order features probably include:\n",
    "    - **LSTAT** squared\n",
    "    - **DIS** with **NOX**\n",
    "    - **LSTAT** with **RM**\n",
    "    - **INDUS** squared\n",
    "    - **AGE** with **NOX**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "- Some modeling techniques work better if all features are on the same scale, and it also helps with model interpretation, so we standardize each feature to have a mean of $\\mu=0$ and standard deviation of $\\sigma=1$.\n",
    "- We only have 1 categorical feature (Charles River flag) which is already binary, but in general, categorical features need to be encoded as binaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' encode features '''\n",
    "# standardize continuous features\n",
    "colTran = ColumnTransformer(transformers=[('scaler', StandardScaler(), contFeatures)], remainder='passthrough', n_jobs=-1)\n",
    "dataS = pd.DataFrame(data=colTran.fit_transform(data), columns=contFeatures+cateFeatures+[response])\n",
    "\n",
    "# encode CHAS categorical - already binary, so make integers\n",
    "dataS['CHAS'] = dataS['CHAS'].astype(int)\n",
    "\n",
    "# talk\n",
    "display(dataS.head())\n",
    "display(dataS.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "- Feature engineering created many features which are likely not useful.\n",
    "- We use a model-based feature selection procedure to get a subset of these for modeling; at least a specified number of features will be selected.\n",
    "- 5-fold validation is employed to minimize any impact of noise on feature selection; final feature importances are averaged over all validation sets.\n",
    "<br><img src=\"./Selection_103.webp\" alt=\"5-fold\" width=\"300\" height=\"300\"><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' use cross-validated recursive feature eliminiation to select features '''\n",
    "# setup\n",
    "estim = LinearRegression()\n",
    "minFeatures = 10\n",
    "cv = 5\n",
    "\n",
    "# run the procedure\n",
    "features = contFeatures+cateFeatures\n",
    "featureSelector = RFECV(estimator=estim, min_features_to_select=minFeatures, cv=cv, n_jobs=-1)\n",
    "featureSelector.fit(X=dataS[features], y=dataS[response])\n",
    "\n",
    "# selected features\n",
    "features = [f for (s, f) in zip(featureSelector.support_, features) if s]\n",
    "p = len(features)\n",
    "\n",
    "# unselected raw features\n",
    "print('Input Features not Used: %s'%([f for f in res['feature_names'] if f not in features]))\n",
    "\n",
    "# variable uses\n",
    "print('Response Variable: %s'%response)\n",
    "print('%d Potential Predictive Features: %s'%(p, features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- Statisticians would generally say that if a model includes a higher-order feature, any related raw features should also be included; it's uncertain if this is really necessary.\n",
    "- According to this preference, unselected features which would need to be included are: **TAX**, **RAD**, **LSTAT**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Success Looks Like: The data has been thoroughly explored, with relevant characteristics documented, with any implications for analysis. Errors, outliers, missing values have been identified and handled. Features have been appropriately prepared for analysis.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze - form hypothesis, select analytical techniques, build model(s), assess results\n",
    "### Form Hypothesis: design experiment using domain knowledge where available, select response variable, select features to utilize\n",
    "\n",
    "<a href=#top>Go to top</a>\n",
    "<a id=analyze-hypothesize></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis**: there is an identifiable relationship between median house price in Boston and at least one of the gathered data features or their higher-order values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Analytical Technique(s): continuous vs. categorical data, supervised vs. unsupervised learning, clustering vs. classification vs. regression techniques; select method types: optimization, sampling, simple vs. ensemble model, robustness\n",
    "\n",
    "<a href=#top>Go to top</a>\n",
    "<a id=analyze-select></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reponse variable is continuously-valued, which indicates that supervised learning, using a regression-based model is appropriate. For demonstrative purposes, three models will be shown:\n",
    "- Multiple Linear Regression\n",
    "- Decision Tree Regression\n",
    "- Simple Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model(s): partition data for training / testing / evaluation, hyperparameter tuning, model training\n",
    "\n",
    "<a href=#top>Go to top</a>\n",
    "<a id=analyze-build></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "- For model evaluation and hyperparameter tuning, use we cross-validation.\n",
    "    - The dataset is split randomly into 70% training, 30% validation, 10 times independently.\n",
    "    - Model metrics are then computed on the validation sets and averaged over them all.\n",
    "- This helps select a model which generalizes to other data (from the same or similar data-generating process), rather than just perfectly fitting the patterns + random noise in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' cross validation '''\n",
    "# setup\n",
    "prng = 42\n",
    "trainPer = 0.7\n",
    "cvCnt = 10\n",
    "\n",
    "# generate the cv indices\n",
    "#splitter = StratifiedShuffleSplit(n_splits=cvCnt, train_size=trainPer, random_state=prng)\n",
    "splitter = ShuffleSplit(n_splits=cvCnt, train_size=trainPer, random_state=prng)\n",
    "cvs = [(tranIndx, testIndx) for (tranIndx, testIndx) in splitter.split(y=dataS['CHAS'].values, X=np.zeros(n))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Linear Regression\n",
    "- Linear regression doesn't really have any hyperparameters to tune.\n",
    "- Acccordingly, we just fit the model with cross-validation.\n",
    "- Predictions are generated for each validation set to be used later by the model evaluation metrics.\n",
    "\n",
    "<a href=#top>Go to top</a>\n",
    "<a id=linreg></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' linear regression '''\n",
    "# fit the model and make predictions\n",
    "linRegResults = [None]*cvCnt\n",
    "for (indx, (trn, tst)) in enumerate(cvs):\n",
    "    print('Training for Split %d of %d'%(indx+1, cvCnt))\n",
    "    # model object\n",
    "    linReg = LinearRegression(n_jobs=-1)\n",
    "    # fit\n",
    "    linReg.fit(X=dataS.loc[trn, features], y=dataS.loc[trn, response])\n",
    "    # predict on training & testing sets\n",
    "    trnPred = linReg.predict(X=dataS.loc[trn, features])\n",
    "    tstPred = linReg.predict(X=dataS.loc[tst, features])\n",
    "    # save stuff\n",
    "    linRegResults[indx] = [linReg, trnPred, tstPred] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Regression\n",
    "- The decision tree regressor is a form of nonlinear regression.\n",
    "- There are several tunable parameters; three are tuned here:\n",
    "    - splitting criterion - changes the loss function used\n",
    "    - maximum depth - deeper means more complex, and increases risk of overfitting\n",
    "    - minimum samples per leaf - few samples per leave increases risk of overfitting\n",
    "- Predictions are generated for each validation set to be used later by the model evaluation metrics.\n",
    "\n",
    "<a href=#top>Go to top</a>\n",
    "<a id=dectree></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' decision tree regression with hyperparameter tuning '''\n",
    "# setup the parameter grid\n",
    "params = {'criterion':['squared_error', 'absolute_error'], 'max_depth':[None, 5, 10], 'min_samples_leaf':[None, 5, 10, 0.05, 0.1]}\n",
    "\n",
    "# perform hyperparameter tuning and model training\n",
    "GSC = GridSearchCV(estimator=DecisionTreeRegressor(), param_grid=params, cv=cvs, n_jobs=-1, verbose=4)\n",
    "GSC.fit(X=dataS[features], y=dataS[response])\n",
    "print('Best Parameters: %s'%GSC.best_params_)\n",
    "\n",
    "# get the predictions for all cvs, using the best set of parameters\n",
    "decTreeResults = [None]*cvCnt\n",
    "for (indx, (trn, tst)) in enumerate(cvs):\n",
    "    print('Training for Split %d of %d'%(indx+1, cvCnt))\n",
    "    # model object\n",
    "    decTree = deepcopy(GSC.best_estimator_)\n",
    "    # fit\n",
    "    decTree.fit(X=dataS.loc[trn, features], y=dataS.loc[trn, response])\n",
    "    # predict on training & testing sets\n",
    "    trnPred = decTree.predict(X=dataS.loc[trn, features])\n",
    "    tstPred = decTree.predict(X=dataS.loc[tst, features])\n",
    "    # save stuff\n",
    "    decTreeResults[indx] = [decTree, trnPred, tstPred] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Neural Network\n",
    "- A neural network model can generally match complex nonlinear relationships.\n",
    "- It requires many parameters and more training data / time than simpler models.\n",
    "- We just demonstrate it here with 1 cross-validation set.\n",
    "- Neural networks are very powerful, but *not a panacea*.\n",
    "\n",
    "<a href=#top>Go to top</a>\n",
    "<a id=nn></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' setup for data access '''\n",
    "# now make the datasets & dataloaders\n",
    "batchSize = 64\n",
    "\n",
    "# make train and test sets\n",
    "cvIndex = 1\n",
    "trn = cvs[cvIndex][0]; tst = cvs[cvIndex][1]\n",
    "trnX = dataS[features].values[trn,:]\n",
    "tstX = dataS[features].values[tst,:]\n",
    "trnY = np.atleast_2d(dataS[response].values[trn]).T\n",
    "tstY = np.atleast_2d(dataS[response].values[tst]).T\n",
    "\n",
    "# training data is accessed in batches, testing data is not\n",
    "trainData = Data(trnX, trnY)\n",
    "trainLoad = DataLoader(dataset=trainData, batch_size=batchSize, shuffle=True)\n",
    "testData = Data(tstX, tstY)\n",
    "testLoad = DataLoader(dataset=testData, batch_size=len(testData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' define the model & operating parameters '''\n",
    "# define the modeling parameters\n",
    "layers = (p, 16, 32, 64, 32, 16, 8, 1) # input layer size, hidden layer sizes, output layer size\n",
    "activations = (None, 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'relu') # first element should be None; it's for the input layer\n",
    "pDropout = 0.2\n",
    "weightInit = 'kai'\n",
    "\n",
    "# set the learning rate\n",
    "learningRate = 0.01\n",
    "stepSize = 50\n",
    "stepMult = 0.1\n",
    "\n",
    "# setup the network, optimizer, and loss\n",
    "torch.manual_seed(prng)\n",
    "thisNN = myNN(layers, weightInit, pDropout, activations)\n",
    "print(thisNN)\n",
    "print('%d parameters'%np.sum([len(p) for p in thisNN.parameters()]))\n",
    "#thisNN.show()\n",
    "optimizer = torch.optim.SGD(params=thisNN.parameters(), lr=learningRate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=stepSize, gamma=stepMult)\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "# define the tensorboard writer\n",
    "writr = SummaryWriter(log_dir='./neuralnet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' train the neural network '''\n",
    "# setup\n",
    "epochs = 1000\n",
    "talkFreq = 0.1\n",
    "\n",
    "# untrained performance\n",
    "thisNN.train(False)\n",
    "print('Pre-training Loss = %0.2f'%loss(thisNN(testLoad.dataset.x), testLoad.dataset.y).detach().numpy())\n",
    "\n",
    "# train\n",
    "trnLoss, tstLoss  = trainNN([trainLoad, testLoad], thisNN, loss, optimizer, scheduler, epochs, writr, None, talkFreq)\n",
    "#thisNN.show()\n",
    "\n",
    "# get the predictions\n",
    "nnPred = thisNN(torch.FloatTensor(dataS[features].values)).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess Results: Numerical evaluation with appropriate metrics, evaluation of model results vs. known responses, feature importance, detailed error (residual) analysis, identification of spurious correlation, confounding features, and need for additional data, comparison to simpler / other / no models, domain-specific assessments,  evaluation of substantiality vis-a-vis business needs; preparation of publication-ready statistical tables and result visualizations\n",
    "\n",
    "<a href=#top>Go to top</a>\n",
    "<a id=analyze-assess></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "- We evaluate several metrics:\n",
    "    - mean squared error: MSE is often used to measure regression model fits\n",
    "    - mean absolute percentage error: MAPE is easily interpretable as the average error relative to average response value\n",
    "    - r squared: $r^2$ measures the linear correlation between actual response and predicted values, closer to 1.0 is better\n",
    "    - root mean squared error: RMSE is the square root of the MSE; it is easily interpretable, as the units of error is the same as the units of the response\n",
    "- All metrics here should be minimized, except for $r^2$, which should be maximized.\n",
    "- Other choices for metrics could be made.\n",
    "- While we measure all metrics, RMSE is taken as the *primary* metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' review error metrics '''\n",
    "# metrics\n",
    "mainMetric = 'rmse' # this is the most important metric - will use later\n",
    "metrics = {'mse':mean_squared_error, 'mape':mean_absolute_percentage_error, 'r2':r2_score, 'rmse':RMSE}\n",
    "template = dict.fromkeys(metrics.keys(), np.nan)\n",
    "resultsTranLR = {}\n",
    "resultsTestLR = {}\n",
    "resultsTranDT = {}\n",
    "resultsTestDT = {}\n",
    "\n",
    "# compute all metrics\n",
    "for indx in range(len(cvs)):\n",
    "    # linear Regression\n",
    "    resKeyLR = 'Linear Regression %d'%indx\n",
    "    resultsTranLR[resKeyLR] = deepcopy(template)\n",
    "    resultsTestLR[resKeyLR] = deepcopy(template)\n",
    "    # Decision Tree\n",
    "    resKeyDT = 'Decision Tree %d'%indx    \n",
    "    resultsTranDT[resKeyDT] = deepcopy(template)\n",
    "    resultsTestDT[resKeyDT] = deepcopy(template)\n",
    "    for metric in metrics.keys():\n",
    "        # linear regression\n",
    "        resultsTranLR[resKeyLR][metric] = metrics[metric](y_true=dataS[response].values[cvs[indx][0]], y_pred=linRegResults[indx][1])\n",
    "        resultsTestLR[resKeyLR][metric] = metrics[metric](y_true=dataS[response].values[cvs[indx][1]], y_pred=linRegResults[indx][2])\n",
    "        # decision tree\n",
    "        resultsTranDT[resKeyDT][metric] = metrics[metric](y_true=dataS[response].values[cvs[indx][0]], y_pred=decTreeResults[indx][1])\n",
    "        resultsTestDT[resKeyDT][metric] = metrics[metric](y_true=dataS[response].values[cvs[indx][1]], y_pred=decTreeResults[indx][2])\n",
    "        \n",
    "# put all together as some dataframes\n",
    "metricsLR = pd.DataFrame(resultsTranLR).T.join(pd.DataFrame(resultsTestLR).T, lsuffix='_train', rsuffix='_test')\n",
    "metricsLR = pd.concat([metricsLR, pd.DataFrame(metricsLR.min(), columns=['Linear Regression Min']).T,\n",
    "                       pd.DataFrame(metricsLR.mean(), columns=['Linear Regression Mean']).T,\n",
    "                       pd.DataFrame(metricsLR.max(), columns=['Linear Regression Max']).T])\n",
    "metricsDT = pd.DataFrame(resultsTranDT).T.join(pd.DataFrame(resultsTestDT).T, lsuffix='_train', rsuffix='_test')\n",
    "metricsDT = pd.concat([metricsDT, pd.DataFrame(metricsDT.min(), columns=['Decision Tree Min']).T,\n",
    "                       pd.DataFrame(metricsDT.mean(), columns=['Decision Tree Mean']).T,\n",
    "                       pd.DataFrame(metricsDT.max(), columns=['Decision Tree Max']).T])\n",
    "\n",
    "# talk\n",
    "print('Linear Regression Metrics')\n",
    "display(metricsLR)\n",
    "print('Decision Tree Metrics')\n",
    "display(metricsDT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- The range of RMSEs for *Linear Regression* overlaps with that for the *Decision Tree* model.\n",
    "- The average RMSE for *Linear Regression* is less than that for the *Decision Tree* model (same for all metrics).\n",
    "- Considering RMSEs, I would select *Linear Regression* as the best model type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "An important part of modeling is evaluating how important each feature is, especially relative to other features:\n",
    "- For regression models, the model coefficients can be interpreted as how much the response should change for a unit change in each feature.\n",
    "- If features are normalized to the same scale, linear regression coefficients are also interpretable as feature importances.\n",
    "- Decision Trees infer feature importance as a function of the decrease in node impurity (for a node that is split based on each feature) and the probability of reaching that node.\n",
    "- More generally, Shapley values can attribute model performance to features in an additive way. The idea of Shapley values comes from cooperative game theory, in which the gain generated by a group of actors is distributed among them.\n",
    "\n",
    "In assessing and diagnosing regression models, there are several standard visualization tools we review:\n",
    "- all are based on predicted values and / or residuals (difference between actual responses and predictions)\n",
    "- *Actual Response Values (X) vs. Predictions*: would like to see a positive linear correlation, with the datapoints narrowly dispersed around the fit line\n",
    "- *Histogram of Residuals*: linear regression assumes residuals are iid Gaussian white noise with constant variance\n",
    "- *Sequence / Time (X) vs. Residuals*: any pattern would suggest there is a time or sequence pattern in the response values, which has not been accounted for\n",
    "- *Actual Response Values (X) vs. Residuals*: would like to see the same dispersion of residuals no matter how large / small the actual response values are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' best linear regression '''\n",
    "# best important metric value\n",
    "vals = metricsLR[mainMetric+'_test'][:cvCnt].values\n",
    "bestIndex = np.argmin(vals)\n",
    "print('Best Linear Regression Model Test %s = %0.2f'%(mainMetric, vals[bestIndex]))\n",
    "\n",
    "# model coefficients\n",
    "bestModel = linRegResults[bestIndex][0]\n",
    "coefs = pd.DataFrame(data=bestModel.coef_, index=features, columns=['Coefficient'])\n",
    "coefs['abs'] = coefs['Coefficient'].abs()\n",
    "coefs.sort_values(by='abs', ascending=False, inplace=True)\n",
    "coefs = pd.concat([pd.DataFrame(index=['Intercept'], data={'Coefficient':[bestModel.intercept_]}), coefs.drop(columns=['abs'])])\n",
    "display(coefs)\n",
    "\n",
    "# prep the dataframe of actuals & predicteds\n",
    "predRes = pd.DataFrame(dataS[response], columns=[response])\n",
    "predRes.loc[cvs[bestIndex][0], 'color'] = 'red'\n",
    "predRes.loc[cvs[bestIndex][1], 'color'] = 'green'\n",
    "predRes.loc[cvs[bestIndex][0], 'Prediction'] = linRegResults[bestIndex][1]\n",
    "predRes.loc[cvs[bestIndex][1], 'Prediction'] = linRegResults[bestIndex][2]\n",
    "predRes['Error'] = predRes[response] - predRes['Prediction']\n",
    "\n",
    "# see the model results plot\n",
    "fig = ResultsPlots(predRes, None, response, 'Prediction', 'Error', 'color', 'Best Linear Regression Model Result - Test %s = %02f'%(mainMetric, vals[bestIndex]))\n",
    "plyoff.iplot(fig)#, include_mathjax='cdn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' best decision tree '''\n",
    "# best important metric value\n",
    "vals = metricsDT[mainMetric+'_test'][:cvCnt].values\n",
    "bestIndex = np.argmin(vals)\n",
    "print('Best Decision Tree Model Test %s = %0.2f'%(mainMetric, vals[bestIndex]))\n",
    "\n",
    "# model coefficients\n",
    "bestModel = decTreeResults[bestIndex][0]\n",
    "featImports = pd.DataFrame(data=bestModel.feature_importances_, index=features, columns=['Importance']).sort_values(by='Importance', ascending=False)\n",
    "display(featImports)\n",
    "\n",
    "# prep the dataframe of actuals & predicteds\n",
    "predRes = pd.DataFrame(dataS[response], columns=[response])\n",
    "predRes.loc[cvs[bestIndex][0], 'color'] = 'red'\n",
    "predRes.loc[cvs[bestIndex][1], 'color'] = 'green'\n",
    "predRes.loc[cvs[bestIndex][0], 'Prediction'] = decTreeResults[bestIndex][1]\n",
    "predRes.loc[cvs[bestIndex][1], 'Prediction'] = decTreeResults[bestIndex][2]\n",
    "predRes['Error'] = predRes[response] - predRes['Prediction']\n",
    "\n",
    "# see the model results plot\n",
    "fig = ResultsPlots(predRes, None, response, 'Prediction', 'Error', 'color', 'Best Decision Tree Model Result - Test %s = %02f'%(mainMetric, vals[bestIndex]))\n",
    "plyoff.iplot(fig)#, include_mathjax='cdn')\n",
    "\n",
    "print('Best Tree')\n",
    "res = plot_tree(decTreeResults[bestIndex][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Neural Network '''\n",
    "# best important metric value\n",
    "nnRMSE = metrics[mainMetric](testLoad.dataset.y, thisNN(testLoad.dataset.x).detach().numpy())\n",
    "print('Neural Network Test %s = %0.2f'%(mainMetric, nnRMSE))\n",
    "\n",
    "# feature importances\n",
    "att = DeepLift(thisNN)\n",
    "#att = IntegratedGradients(thisNN)\n",
    "att = NoiseTunnel(att)\n",
    "baseline = torch.zeros(testLoad.dataset.x.shape[0], testLoad.dataset.x.shape[1]) \n",
    "attributions, delta = att.attribute(inputs=testLoad.dataset.x, baselines=baseline, target=0, return_convergence_delta=True)\n",
    "featImports = pd.DataFrame(np.mean(attributions.detach().numpy(), axis=0).T, index=features, columns=['Importance'])\n",
    "featImports['abs'] = featImports['Importance'].abs()\n",
    "featImports.sort_values(by='abs', ascending=False, inplace=True)\n",
    "featImports.drop(columns=['abs'], inplace=True)\n",
    "display(featImports)\n",
    "\n",
    "# prep the dataframe of actuals & predicteds\n",
    "predRes = pd.DataFrame(dataS[response], columns=[response])\n",
    "predRes.loc[cvs[0][0], 'color'] = 'red'\n",
    "predRes.loc[cvs[0][1], 'color'] = 'green'\n",
    "predRes['Prediction'] = nnPred\n",
    "predRes['Error'] = predRes[response] - predRes['Prediction']\n",
    "\n",
    "# see the model results plot\n",
    "fig = ResultsPlots(predRes, None, response, 'Prediction', 'Error', 'color', 'Neural Network Model Result - Test %s = %02f'%(mainMetric, nnRMSE))\n",
    "plyoff.iplot(fig)#, include_mathjax='cdn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "Importances:\n",
    "- All three also models show some significant differences in most important features - but also similarities.\n",
    "- *Linear Regression*: **PTRATIO_TAX**, **RM**, **AGE**\n",
    "- *Decision Tree*: **RM**, **AGE_LSTAT**, **LSTAT_sq**\n",
    "- *Neural Network*: **LSTAT_TAX**, **RM**, **AGE_LSTAT**\n",
    "\n",
    "Diagnostic Plots:\n",
    "- All three of these models have issues showing in residual plots, which would need to be evaluated.\n",
    "- *Linear Regression*: strange vertical line of predictions & residuals at the maximum response value; suggests some response value data truncation\n",
    "- *Decision Tree*: horizontal lines in 2 plots indicate the decision tree might be overly simple, as ranges of responses have the same predictions\n",
    "- *Neural Network*: odd patterns visible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Success Looks Like: A relevant and testable hypothesis has been formed and analyzed using the appropriate modeling techniques. Model results have been assessed using appropriate metrics.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report - Document the APA portion of the BAPARA process followed as relevant to audience and context, assumptions made and the degree to which they were validated (or not), shortcomings in the data, and how they may be overcome, potential ways to improve the result, generalizability of the model(s), importance & value to the business; this is when peer review should be performed\n",
    "- Note that a negative result - i.e. a model that does not perform good enough - should still be reported. Potential value remains in how the process was followed (code, data), how models were run, and the results.\n",
    "- *Success Looks Like: Assumptions, decisions, process, results, and implications accurately and effectively documented.* \n",
    "\n",
    "## Act - Realize the value proposition of the project by applying the insight in the workflow, evaluate effectiveness in context and identify the source of the ultimate value; if operationalization is needed, this can be almost as involved as the entire APA portion of the BAPARA process\n",
    "\n",
    "<a href=#top>Go to top</a>\n",
    "<a id=report-act></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The end\n",
    "\n",
    "<a href=#top>Go to top</a>\n",
    "<a id=end></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
